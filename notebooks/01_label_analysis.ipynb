{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 01: Label Analysis\n",
        "\n",
        "**Purpose**: Deeply understand the labels we're trying to predict\n",
        "\n",
        "**Critical Questions**:\n",
        "1. Is the current horizon (50 samples) optimal?\n",
        "2. Is the current threshold (8 bps) optimal?\n",
        "3. Are labels clustered (autocorrelated) or random?\n",
        "4. What are the transition probabilities between labels?\n",
        "\n",
        "---\n",
        "\n",
        "## Label Configuration\n",
        "\n",
        "Current configuration (from `nvda_98feat.toml`):\n",
        "- **Horizon**: 50 samples ahead\n",
        "- **Smoothing**: 10-sample window\n",
        "- **Threshold**: 8 bps (0.08%)\n",
        "- **Labels**: -1=Down (>8bps drop), 0=Stable, 1=Up (>8bps rise)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from scipy import stats\n",
        "from collections import Counter\n",
        "import json\n",
        "import sys\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
        "\n",
        "from lobtrainer.constants import (\n",
        "    FEATURE_COUNT, FeatureIndex,\n",
        "    LABEL_DOWN, LABEL_STABLE, LABEL_UP, LABEL_NAMES\n",
        ")\n",
        "\n",
        "# Plotting configuration\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# Data path\n",
        "DATA_ROOT = Path.cwd().parent.parent / 'data' / 'exports' / 'nvda_98feat'\n",
        "\n",
        "print(\"Environment ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load training data\n",
        "def load_split(split_name):\n",
        "    \"\"\"Load all data for a split.\"\"\"\n",
        "    split_dir = DATA_ROOT / split_name\n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "    metadata_list = []\n",
        "    \n",
        "    for feat_file in sorted(split_dir.glob('*_features.npy')):\n",
        "        date = feat_file.stem.replace('_features', '')\n",
        "        label_file = feat_file.parent / f\"{date}_labels.npy\"\n",
        "        meta_file = feat_file.parent / f\"{date}_metadata.json\"\n",
        "        \n",
        "        features_list.append(np.load(feat_file))\n",
        "        labels_list.append(np.load(label_file))\n",
        "        \n",
        "        if meta_file.exists():\n",
        "            with open(meta_file) as f:\n",
        "                metadata_list.append(json.load(f))\n",
        "    \n",
        "    return {\n",
        "        'features': np.vstack(features_list),\n",
        "        'labels': np.concatenate(labels_list),\n",
        "        'metadata': metadata_list,\n",
        "        'n_days': len(features_list),\n",
        "    }\n",
        "\n",
        "train_data = load_split('train')\n",
        "print(f\"Training data: {train_data['features'].shape[0]:,} samples, {len(train_data['labels']):,} labels, {train_data['n_days']} days\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Label Distribution Analysis\n",
        "\n",
        "How balanced are the labels? Is there class imbalance we need to address?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = train_data['labels']\n",
        "\n",
        "# Basic distribution\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "total = len(labels)\n",
        "\n",
        "print(\"Label Distribution:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "label_dist = {}\n",
        "for lbl, cnt in zip(unique, counts):\n",
        "    name = LABEL_NAMES.get(int(lbl), f\"Unknown({lbl})\")\n",
        "    pct = 100 * cnt / total\n",
        "    label_dist[int(lbl)] = {'count': cnt, 'pct': pct, 'name': name}\n",
        "    print(f\"  {name:8s} (label={int(lbl):2d}): {cnt:7,} ({pct:5.2f}%)\")\n",
        "\n",
        "# Class imbalance metrics\n",
        "max_count = max(counts)\n",
        "min_count = min(counts)\n",
        "imbalance_ratio = max_count / min_count\n",
        "\n",
        "print(f\"\\nClass Imbalance Metrics:\")\n",
        "print(f\"  Max/Min ratio: {imbalance_ratio:.3f}\")\n",
        "print(f\"  Majority class: {LABEL_NAMES[int(unique[np.argmax(counts)])]}\")\n",
        "print(f\"  Minority class: {LABEL_NAMES[int(unique[np.argmin(counts)])]}\")\n",
        "\n",
        "if imbalance_ratio < 1.5:\n",
        "    print(f\"  ✅ Labels are well-balanced (ratio < 1.5)\")\n",
        "elif imbalance_ratio < 2.0:\n",
        "    print(f\"  ⚠️ Slight imbalance (consider weighting)\")\n",
        "else:\n",
        "    print(f\"  ❌ Significant imbalance (requires class weighting or resampling)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize label distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Bar chart\n",
        "colors = ['#e74c3c', '#95a5a6', '#27ae60']  # Red, Gray, Green\n",
        "ax1 = axes[0]\n",
        "bars = ax1.bar([LABEL_NAMES[lbl] for lbl in unique], counts, color=colors, edgecolor='black')\n",
        "ax1.set_xlabel('Label')\n",
        "ax1.set_ylabel('Count')\n",
        "ax1.set_title('Label Distribution')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, cnt in zip(bars, counts):\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 500,\n",
        "             f'{cnt:,}\\n({100*cnt/total:.1f}%)', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "# Pie chart\n",
        "ax2 = axes[1]\n",
        "wedges, texts, autotexts = ax2.pie(counts, labels=[LABEL_NAMES[lbl] for lbl in unique],\n",
        "                                    colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "ax2.set_title('Label Proportions')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../docs/figures/label_distribution.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nRandom baseline accuracy: {100/len(unique):.1f}% (predicting uniformly)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Label Autocorrelation\n",
        "\n",
        "Are labels clustered (trends persist) or random? This affects:\n",
        "- Whether sequence models will help\n",
        "- Whether we should predict sequences of labels\n",
        "- Potential for momentum strategies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_autocorrelation(series, max_lag=50):\n",
        "    \"\"\"\n",
        "    Compute autocorrelation function for a series.\n",
        "    \n",
        "    ACF(k) = Corr(X_t, X_{t+k})\n",
        "    \n",
        "    Returns:\n",
        "        lags: array of lag values\n",
        "        acf: autocorrelation at each lag\n",
        "    \"\"\"\n",
        "    n = len(series)\n",
        "    series = series.astype(float)\n",
        "    mean = series.mean()\n",
        "    var = series.var()\n",
        "    \n",
        "    if var == 0:\n",
        "        return np.arange(max_lag + 1), np.ones(max_lag + 1)\n",
        "    \n",
        "    acf = np.zeros(max_lag + 1)\n",
        "    acf[0] = 1.0  # Correlation with self\n",
        "    \n",
        "    for lag in range(1, max_lag + 1):\n",
        "        if lag >= n:\n",
        "            break\n",
        "        # Compute correlation between series[:-lag] and series[lag:]\n",
        "        cov = np.mean((series[:-lag] - mean) * (series[lag:] - mean))\n",
        "        acf[lag] = cov / var\n",
        "    \n",
        "    return np.arange(max_lag + 1), acf\n",
        "\n",
        "# Compute autocorrelation\n",
        "max_lag = 100\n",
        "lags, acf = compute_autocorrelation(labels, max_lag=max_lag)\n",
        "\n",
        "# 95% confidence interval for white noise: ±1.96/sqrt(n)\n",
        "ci = 1.96 / np.sqrt(len(labels))\n",
        "\n",
        "print(\"Label Autocorrelation Analysis:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Sample size: {len(labels):,}\")\n",
        "print(f\"95% CI for white noise: ±{ci:.4f}\")\n",
        "print(f\"\\nAutocorrelation at key lags:\")\n",
        "for lag in [1, 5, 10, 20, 50]:\n",
        "    if lag < len(acf):\n",
        "        sig = \"***\" if abs(acf[lag]) > ci else \"\"\n",
        "        print(f\"  Lag {lag:3d}: {acf[lag]:+.4f} {sig}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize autocorrelation\n",
        "fig, ax = plt.subplots(figsize=(14, 5))\n",
        "\n",
        "ax.bar(lags, acf, color='steelblue', alpha=0.7, width=0.8)\n",
        "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "ax.axhline(y=ci, color='red', linestyle='--', alpha=0.7, label=f'95% CI (±{ci:.4f})')\n",
        "ax.axhline(y=-ci, color='red', linestyle='--', alpha=0.7)\n",
        "\n",
        "ax.set_xlabel('Lag')\n",
        "ax.set_ylabel('Autocorrelation')\n",
        "ax.set_title('Label Autocorrelation Function (ACF)')\n",
        "ax.legend()\n",
        "ax.set_xlim(-1, max_lag + 1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../docs/figures/label_autocorrelation.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Interpretation\n",
        "if acf[1] > 0.1:\n",
        "    print(\"\\n✅ Strong positive autocorrelation: Labels cluster (trends persist)\")\n",
        "    print(\"   → Sequence models may capture momentum patterns\")\n",
        "elif acf[1] > ci:\n",
        "    print(\"\\n⚠️ Weak positive autocorrelation: Some label persistence\")\n",
        "    print(\"   → Sequence information may help slightly\")\n",
        "elif acf[1] < -ci:\n",
        "    print(\"\\n⚠️ Negative autocorrelation: Mean-reversion in labels\")\n",
        "    print(\"   → Consider mean-reversion strategies\")\n",
        "else:\n",
        "    print(\"\\n❌ No significant autocorrelation: Labels appear random\")\n",
        "    print(\"   → Sequence models may not add value over point predictions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Transition Matrix Analysis\n",
        "\n",
        "What are the probabilities of transitioning between labels?\n",
        "\n",
        "- High P(Up→Up) suggests momentum\n",
        "- High P(Up→Down) suggests mean-reversion\n",
        "- Equal probabilities suggest random walk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_transition_matrix(labels):\n",
        "    \"\"\"\n",
        "    Compute transition probability matrix for labels.\n",
        "    \n",
        "    P[i, j] = P(label_{t+1} = j | label_t = i)\n",
        "    \n",
        "    Returns:\n",
        "        transition_counts: raw counts\n",
        "        transition_probs: normalized probabilities\n",
        "        label_order: order of labels in matrix\n",
        "    \"\"\"\n",
        "    label_order = sorted(np.unique(labels))\n",
        "    n_labels = len(label_order)\n",
        "    label_to_idx = {lbl: i for i, lbl in enumerate(label_order)}\n",
        "    \n",
        "    # Count transitions\n",
        "    counts = np.zeros((n_labels, n_labels), dtype=int)\n",
        "    for i in range(len(labels) - 1):\n",
        "        from_idx = label_to_idx[labels[i]]\n",
        "        to_idx = label_to_idx[labels[i + 1]]\n",
        "        counts[from_idx, to_idx] += 1\n",
        "    \n",
        "    # Normalize to probabilities\n",
        "    row_sums = counts.sum(axis=1, keepdims=True)\n",
        "    probs = np.divide(counts, row_sums, where=row_sums > 0)\n",
        "    \n",
        "    return counts, probs, label_order\n",
        "\n",
        "counts, probs, label_order = compute_transition_matrix(labels)\n",
        "\n",
        "print(\"Transition Matrix Analysis:\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nTransition Counts:\")\n",
        "print(f\"{'From \\\\ To':>10s}\", end='')\n",
        "for lbl in label_order:\n",
        "    print(f\"{LABEL_NAMES[lbl]:>10s}\", end='')\n",
        "print()\n",
        "for i, from_lbl in enumerate(label_order):\n",
        "    print(f\"{LABEL_NAMES[from_lbl]:>10s}\", end='')\n",
        "    for j in range(len(label_order)):\n",
        "        print(f\"{counts[i, j]:>10,}\", end='')\n",
        "    print()\n",
        "\n",
        "print(\"\\nTransition Probabilities:\")\n",
        "print(f\"{'From \\\\ To':>10s}\", end='')\n",
        "for lbl in label_order:\n",
        "    print(f\"{LABEL_NAMES[lbl]:>10s}\", end='')\n",
        "print()\n",
        "for i, from_lbl in enumerate(label_order):\n",
        "    print(f\"{LABEL_NAMES[from_lbl]:>10s}\", end='')\n",
        "    for j in range(len(label_order)):\n",
        "        print(f\"{probs[i, j]:>10.3f}\", end='')\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize transition matrix\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Probability heatmap\n",
        "ax1 = axes[0]\n",
        "label_names_ordered = [LABEL_NAMES[lbl] for lbl in label_order]\n",
        "sns.heatmap(probs, annot=True, fmt='.3f', cmap='Blues', ax=ax1,\n",
        "            xticklabels=label_names_ordered, yticklabels=label_names_ordered,\n",
        "            vmin=0, vmax=1)\n",
        "ax1.set_xlabel('To')\n",
        "ax1.set_ylabel('From')\n",
        "ax1.set_title('Transition Probabilities P(To | From)')\n",
        "\n",
        "# Compare to stationary (random) probabilities\n",
        "ax2 = axes[1]\n",
        "stationary_probs = np.array([label_dist[lbl]['pct']/100 for lbl in label_order])\n",
        "expected_probs = np.outer(np.ones(len(label_order)), stationary_probs)\n",
        "diff = probs - expected_probs\n",
        "\n",
        "sns.heatmap(diff, annot=True, fmt='+.3f', cmap='RdBu_r', ax=ax2,\n",
        "            xticklabels=label_names_ordered, yticklabels=label_names_ordered,\n",
        "            center=0, vmin=-0.1, vmax=0.1)\n",
        "ax2.set_xlabel('To')\n",
        "ax2.set_ylabel('From')\n",
        "ax2.set_title('Deviation from Stationary Distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../docs/figures/transition_matrix.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Interpretation\n",
        "print(\"\\nKey Insights:\")\n",
        "diagonal = np.diag(probs)\n",
        "for i, lbl in enumerate(label_order):\n",
        "    name = LABEL_NAMES[lbl]\n",
        "    stationary = stationary_probs[i]\n",
        "    persist = diagonal[i]\n",
        "    if persist > stationary + 0.05:\n",
        "        print(f\"  • {name} persists: P({name}→{name}) = {persist:.1%} > stationary {stationary:.1%}\")\n",
        "    elif persist < stationary - 0.05:\n",
        "        print(f\"  • {name} reverts: P({name}→{name}) = {persist:.1%} < stationary {stationary:.1%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Signal-Label Correlation\n",
        "\n",
        "Quick preview of which signals correlate with labels (detailed analysis in Notebook 04).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Align features with labels\n",
        "# Features are at higher frequency; labels align with sequence ends\n",
        "features = train_data['features']\n",
        "stride = 10  # From config\n",
        "window = 100  # From config\n",
        "\n",
        "# Sample features at label positions (end of each sequence)\n",
        "n_labels = len(labels)\n",
        "aligned_features = []\n",
        "\n",
        "for i in range(n_labels):\n",
        "    # The label for sequence i corresponds to features ending at position (i+1)*stride + window - stride\n",
        "    feat_end_idx = min((i + 1) * stride + window - stride, features.shape[0] - 1)\n",
        "    aligned_features.append(features[feat_end_idx])\n",
        "\n",
        "aligned_features = np.array(aligned_features)\n",
        "print(f\"Aligned {len(aligned_features)} feature samples with {len(labels)} labels\")\n",
        "\n",
        "# Signal indices and names\n",
        "signal_indices = {\n",
        "    84: 'true_ofi',\n",
        "    85: 'depth_norm_ofi',\n",
        "    86: 'executed_pressure',\n",
        "    87: 'signed_mp_delta_bps',\n",
        "    88: 'trade_asymmetry',\n",
        "    89: 'cancel_asymmetry',\n",
        "    90: 'fragility_score',\n",
        "    91: 'depth_asymmetry',\n",
        "}\n",
        "\n",
        "# Compute correlations\n",
        "print(\"\\nSignal-Label Correlations:\")\n",
        "print(\"=\" * 50)\n",
        "correlations = {}\n",
        "for idx, name in signal_indices.items():\n",
        "    signal = aligned_features[:, idx]\n",
        "    corr = np.corrcoef(signal, labels)[0, 1]\n",
        "    correlations[name] = corr\n",
        "    significance = \"***\" if abs(corr) > 0.05 else \"**\" if abs(corr) > 0.02 else \"*\" if abs(corr) > 0.01 else \"\"\n",
        "    print(f\"  {name:25s}: {corr:+.4f} {significance}\")\n",
        "\n",
        "# Sort by absolute correlation\n",
        "sorted_corrs = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
        "print(f\"\\nTop Predictors:\")\n",
        "for name, corr in sorted_corrs[:5]:\n",
        "    print(f\"  {name:25s}: {corr:+.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Label Dynamics by Time Regime\n",
        "\n",
        "Do label patterns vary by market session?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time regime at each label position\n",
        "time_regimes = aligned_features[:, FeatureIndex.TIME_REGIME]\n",
        "\n",
        "regime_names = {\n",
        "    0: 'Open (9:30-9:45)',\n",
        "    1: 'Early (9:45-10:30)',\n",
        "    2: 'Midday (10:30-15:30)',\n",
        "    3: 'Close (15:30-16:00)',\n",
        "    4: 'Closed'\n",
        "}\n",
        "\n",
        "print(\"Label Distribution by Time Regime:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "regime_stats = []\n",
        "for regime in sorted(np.unique(time_regimes)):\n",
        "    mask = time_regimes == regime\n",
        "    regime_labels = labels[mask]\n",
        "    \n",
        "    if len(regime_labels) < 100:\n",
        "        continue\n",
        "    \n",
        "    up_pct = 100 * (regime_labels == LABEL_UP).mean()\n",
        "    down_pct = 100 * (regime_labels == LABEL_DOWN).mean()\n",
        "    stable_pct = 100 * (regime_labels == LABEL_STABLE).mean()\n",
        "    \n",
        "    # Compute signal-label correlation in this regime\n",
        "    regime_ofi = aligned_features[mask, 84]  # true_ofi\n",
        "    ofi_corr = np.corrcoef(regime_ofi, regime_labels)[0, 1]\n",
        "    \n",
        "    regime_stats.append({\n",
        "        'regime': int(regime),\n",
        "        'name': regime_names.get(int(regime), 'Unknown'),\n",
        "        'n_samples': len(regime_labels),\n",
        "        'up_pct': up_pct,\n",
        "        'down_pct': down_pct,\n",
        "        'stable_pct': stable_pct,\n",
        "        'ofi_corr': ofi_corr,\n",
        "    })\n",
        "    \n",
        "    print(f\"\\n{regime_names.get(int(regime), 'Unknown')} (n={len(regime_labels):,}):\")\n",
        "    print(f\"  Up: {up_pct:.1f}%, Down: {down_pct:.1f}%, Stable: {stable_pct:.1f}%\")\n",
        "    print(f\"  true_ofi correlation: {ofi_corr:+.4f}\")\n",
        "\n",
        "df_regimes = pd.DataFrame(regime_stats)\n",
        "print(\"\\n\" + df_regimes.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary & Key Findings\n",
        "\n",
        "Synthesize label analysis findings to guide modeling decisions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"LABEL ANALYSIS SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\"\"\n",
        "1. LABEL DISTRIBUTION\n",
        "   • Total labels: {len(labels):,}\n",
        "   • Class balance: Max/Min ratio = {imbalance_ratio:.2f}\n",
        "   • Down: {label_dist[LABEL_DOWN]['pct']:.1f}%, Stable: {label_dist[LABEL_STABLE]['pct']:.1f}%, Up: {label_dist[LABEL_UP]['pct']:.1f}%\n",
        "   • Assessment: {'Well-balanced' if imbalance_ratio < 1.5 else 'Needs class weighting'}\n",
        "\n",
        "2. AUTOCORRELATION\n",
        "   • Lag-1 ACF: {acf[1]:.4f}\n",
        "   • Lag-5 ACF: {acf[5]:.4f}\n",
        "   • Assessment: {'Labels cluster (momentum)' if acf[1] > 0.05 else 'Labels appear random' if abs(acf[1]) < ci else 'Labels mean-revert'}\n",
        "\n",
        "3. TRANSITION DYNAMICS\n",
        "   • P(Up→Up): {probs[label_order.index(LABEL_UP), label_order.index(LABEL_UP)]:.3f}\n",
        "   • P(Down→Down): {probs[label_order.index(LABEL_DOWN), label_order.index(LABEL_DOWN)]:.3f}\n",
        "   • Assessment: {'Momentum patterns present' if np.diag(probs).mean() > 0.4 else 'Near-random transitions'}\n",
        "\n",
        "4. SIGNAL PREDICTABILITY\n",
        "   • Best predictor: {sorted_corrs[0][0]} (r={sorted_corrs[0][1]:+.4f})\n",
        "   • Second best: {sorted_corrs[1][0]} (r={sorted_corrs[1][1]:+.4f})\n",
        "   • Assessment: {'Signals have predictive power' if abs(sorted_corrs[0][1]) > 0.05 else 'Weak signal-label relationship'}\n",
        "\n",
        "5. REGIME EFFECTS\n",
        "   • Strongest OFI correlation: {max(df_regimes['ofi_corr']):.4f} in {df_regimes.loc[df_regimes['ofi_corr'].idxmax(), 'name']}\n",
        "   • Weakest OFI correlation: {min(df_regimes['ofi_corr']):.4f} in {df_regimes.loc[df_regimes['ofi_corr'].idxmin(), 'name']}\n",
        "   • Assessment: {'Regime-specific models may help' if df_regimes['ofi_corr'].std() > 0.02 else 'Uniform predictability across regimes'}\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"RECOMMENDATIONS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "recommendations = []\n",
        "if imbalance_ratio < 1.5:\n",
        "    recommendations.append(\"✅ Use standard cross-entropy loss (balanced classes)\")\n",
        "else:\n",
        "    recommendations.append(\"⚠️ Use class-weighted loss or oversampling\")\n",
        "\n",
        "if acf[1] > 0.05:\n",
        "    recommendations.append(\"✅ Sequence models (LSTM/Transformer) likely beneficial\")\n",
        "else:\n",
        "    recommendations.append(\"⚠️ Point prediction models may suffice\")\n",
        "\n",
        "if abs(sorted_corrs[0][1]) > 0.05:\n",
        "    recommendations.append(f\"✅ Focus on {sorted_corrs[0][0]} as primary feature\")\n",
        "else:\n",
        "    recommendations.append(\"⚠️ Need feature engineering or more signals\")\n",
        "\n",
        "for rec in recommendations:\n",
        "    print(f\"  {rec}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"✅ LABEL ANALYSIS COMPLETE\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save analysis results\n",
        "import os\n",
        "os.makedirs('../docs/figures', exist_ok=True)\n",
        "\n",
        "analysis_results = {\n",
        "    'label_distribution': {\n",
        "        'down': int(label_dist[LABEL_DOWN]['count']),\n",
        "        'stable': int(label_dist[LABEL_STABLE]['count']),\n",
        "        'up': int(label_dist[LABEL_UP]['count']),\n",
        "        'imbalance_ratio': float(imbalance_ratio),\n",
        "    },\n",
        "    'autocorrelation': {\n",
        "        'lag_1': float(acf[1]),\n",
        "        'lag_5': float(acf[5]),\n",
        "        'lag_10': float(acf[10]),\n",
        "        'confidence_interval': float(ci),\n",
        "    },\n",
        "    'transition_matrix': probs.tolist(),\n",
        "    'signal_correlations': {k: float(v) for k, v in correlations.items()},\n",
        "    'top_predictors': [(k, float(v)) for k, v in sorted_corrs[:5]],\n",
        "    'regime_stats': df_regimes.to_dict('records'),\n",
        "}\n",
        "\n",
        "with open('../docs/label_analysis_results.json', 'w') as f:\n",
        "    json.dump(analysis_results, f, indent=2)\n",
        "\n",
        "print(\"Results saved to docs/label_analysis_results.json\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
