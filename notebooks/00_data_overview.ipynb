{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 00: Data Overview & Sanity Checks\n",
        "\n",
        "**Purpose**: Validate data integrity before any analysis\n",
        "\n",
        "**Principle**: Trust but verify - every assumption must be checked\n",
        "\n",
        "---\n",
        "\n",
        "## Objectives\n",
        "\n",
        "1. Load and validate all exported data files\n",
        "2. Verify feature count matches contract (98 features)\n",
        "3. Check data types, NaN, Inf, missing values\n",
        "4. Validate categorical features (book_valid, time_regime, mbo_ready, schema_version)\n",
        "5. Basic statistics per feature category\n",
        "6. Confirm known relationships hold\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import json\n",
        "import sys\n",
        "\n",
        "# Add src to path for lobtrainer imports\n",
        "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
        "\n",
        "from lobtrainer.constants import (\n",
        "    FEATURE_COUNT, LOB_FEATURE_COUNT, DERIVED_FEATURE_COUNT,\n",
        "    MBO_FEATURE_COUNT, SIGNAL_FEATURE_COUNT, SCHEMA_VERSION,\n",
        "    FeatureIndex, LABEL_DOWN, LABEL_STABLE, LABEL_UP, LABEL_NAMES\n",
        ")\n",
        "\n",
        "# Plotting configuration\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.precision', 6)\n",
        "\n",
        "print(\"Environment ready\")\n",
        "print(f\"Expected feature count: {FEATURE_COUNT}\")\n",
        "print(f\"  LOB: {LOB_FEATURE_COUNT}, Derived: {DERIVED_FEATURE_COUNT}, MBO: {MBO_FEATURE_COUNT}, Signals: {SIGNAL_FEATURE_COUNT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading\n",
        "\n",
        "Load all data files from train/val/test splits and verify structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data directories\n",
        "DATA_ROOT = Path.cwd().parent.parent / 'data' / 'exports' / 'nvda_98feat'\n",
        "SPLITS = ['train', 'val', 'test']\n",
        "\n",
        "print(f\"Data root: {DATA_ROOT}\")\n",
        "print(f\"Exists: {DATA_ROOT.exists()}\")\n",
        "\n",
        "# Discover all files\n",
        "data_inventory = {}\n",
        "for split in SPLITS:\n",
        "    split_dir = DATA_ROOT / split\n",
        "    if not split_dir.exists():\n",
        "        print(f\"Warning: {split} directory not found\")\n",
        "        continue\n",
        "    \n",
        "    feature_files = sorted(split_dir.glob('*_features.npy'))\n",
        "    label_files = sorted(split_dir.glob('*_labels.npy'))\n",
        "    metadata_files = sorted(split_dir.glob('*_metadata.json'))\n",
        "    \n",
        "    data_inventory[split] = {\n",
        "        'feature_files': feature_files,\n",
        "        'label_files': label_files,\n",
        "        'metadata_files': metadata_files,\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n{split.upper()}:\")\n",
        "    print(f\"  Feature files: {len(feature_files)}\")\n",
        "    print(f\"  Label files: {len(label_files)}\")\n",
        "    print(f\"  Metadata files: {len(metadata_files)}\")\n",
        "    \n",
        "    if feature_files:\n",
        "        dates = [f.stem.replace('_features', '') for f in feature_files]\n",
        "        print(f\"  Dates: {dates[0]} to {dates[-1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_day_data(feature_path: Path, label_path: Path, metadata_path: Path = None):\n",
        "    \"\"\"\n",
        "    Load a single day's data with validation.\n",
        "    \n",
        "    Returns:\n",
        "        dict with 'features', 'labels', 'metadata', 'date'\n",
        "    \"\"\"\n",
        "    features = np.load(feature_path)\n",
        "    labels = np.load(label_path)\n",
        "    \n",
        "    metadata = None\n",
        "    if metadata_path and metadata_path.exists():\n",
        "        with open(metadata_path) as f:\n",
        "            metadata = json.load(f)\n",
        "    \n",
        "    date = feature_path.stem.replace('_features', '')\n",
        "    \n",
        "    return {\n",
        "        'features': features,\n",
        "        'labels': labels,\n",
        "        'metadata': metadata,\n",
        "        'date': date,\n",
        "    }\n",
        "\n",
        "# Load all data\n",
        "all_data = {split: [] for split in SPLITS}\n",
        "\n",
        "for split in SPLITS:\n",
        "    if split not in data_inventory:\n",
        "        continue\n",
        "    \n",
        "    inv = data_inventory[split]\n",
        "    for feat_file, label_file in zip(inv['feature_files'], inv['label_files']):\n",
        "        # Find matching metadata file\n",
        "        date = feat_file.stem.replace('_features', '')\n",
        "        meta_file = feat_file.parent / f\"{date}_metadata.json\"\n",
        "        \n",
        "        day_data = load_day_data(feat_file, label_file, meta_file)\n",
        "        all_data[split].append(day_data)\n",
        "\n",
        "print(\"Data loaded successfully\")\n",
        "for split in SPLITS:\n",
        "    if all_data[split]:\n",
        "        print(f\"  {split}: {len(all_data[split])} days\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Shape and Type Validation\n",
        "\n",
        "Verify that all files have the expected structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validation results\n",
        "validation_results = []\n",
        "\n",
        "for split in SPLITS:\n",
        "    for day in all_data[split]:\n",
        "        features = day['features']\n",
        "        labels = day['labels']\n",
        "        \n",
        "        result = {\n",
        "            'split': split,\n",
        "            'date': day['date'],\n",
        "            'n_features': features.shape[0],\n",
        "            'n_feature_dims': features.shape[1],\n",
        "            'n_labels': labels.shape[0],\n",
        "            'feature_dtype': str(features.dtype),\n",
        "            'label_dtype': str(labels.dtype),\n",
        "            'feature_dim_ok': features.shape[1] == FEATURE_COUNT,\n",
        "            'ratio': features.shape[0] / labels.shape[0] if labels.shape[0] > 0 else 0,\n",
        "        }\n",
        "        \n",
        "        validation_results.append(result)\n",
        "\n",
        "df_validation = pd.DataFrame(validation_results)\n",
        "print(\"Shape Validation Summary:\")\n",
        "print(df_validation.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for any validation failures\n",
        "failures = df_validation[~df_validation['feature_dim_ok']]\n",
        "if len(failures) > 0:\n",
        "    print(\"❌ CRITICAL: Feature dimension mismatch detected!\")\n",
        "    print(failures)\n",
        "else:\n",
        "    print(f\"✅ All {len(df_validation)} files have correct feature dimension ({FEATURE_COUNT})\")\n",
        "\n",
        "# Check feature/label ratio consistency\n",
        "ratio_mean = df_validation['ratio'].mean()\n",
        "ratio_std = df_validation['ratio'].std()\n",
        "print(f\"\\nFeature/Label ratio: {ratio_mean:.2f} ± {ratio_std:.2f}\")\n",
        "print(f\"  Expected: ~10 (stride=10, window=100)\")\n",
        "\n",
        "if abs(ratio_mean - 10) > 1:\n",
        "    print(\"  ⚠️ Warning: Ratio deviates from expected value\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Quality Checks\n",
        "\n",
        "Check for NaN, Inf, and other data quality issues.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate all training data for analysis\n",
        "train_features = np.vstack([d['features'] for d in all_data['train']])\n",
        "train_labels = np.concatenate([d['labels'] for d in all_data['train']])\n",
        "\n",
        "print(f\"Training data shape: {train_features.shape}\")\n",
        "print(f\"Training labels shape: {train_labels.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for NaN and Inf\n",
        "nan_count = np.isnan(train_features).sum()\n",
        "inf_count = np.isinf(train_features).sum()\n",
        "finite_count = np.isfinite(train_features).sum()\n",
        "total_count = train_features.size\n",
        "\n",
        "print(\"Data Quality Check:\")\n",
        "print(f\"  Total values: {total_count:,}\")\n",
        "print(f\"  Finite values: {finite_count:,} ({100*finite_count/total_count:.4f}%)\")\n",
        "print(f\"  NaN values: {nan_count:,} ({100*nan_count/total_count:.6f}%)\")\n",
        "print(f\"  Inf values: {inf_count:,} ({100*inf_count/total_count:.6f}%)\")\n",
        "\n",
        "if nan_count > 0 or inf_count > 0:\n",
        "    print(\"\\n❌ WARNING: Non-finite values detected!\")\n",
        "    # Find which features have issues\n",
        "    for col in range(train_features.shape[1]):\n",
        "        col_nan = np.isnan(train_features[:, col]).sum()\n",
        "        col_inf = np.isinf(train_features[:, col]).sum()\n",
        "        if col_nan > 0 or col_inf > 0:\n",
        "            print(f\"    Feature {col}: {col_nan} NaN, {col_inf} Inf\")\n",
        "else:\n",
        "    print(\"\\n✅ All values are finite (no NaN or Inf)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check label values\n",
        "unique_labels, label_counts = np.unique(train_labels, return_counts=True)\n",
        "\n",
        "print(\"Label Distribution:\")\n",
        "for label, count in zip(unique_labels, label_counts):\n",
        "    label_name = LABEL_NAMES.get(int(label), f\"Unknown({label})\")\n",
        "    pct = 100 * count / len(train_labels)\n",
        "    print(f\"  {label_name:8s} (label={int(label):2d}): {count:7,} ({pct:5.2f}%)\")\n",
        "\n",
        "# Verify expected labels\n",
        "expected_labels = {LABEL_DOWN, LABEL_STABLE, LABEL_UP}\n",
        "actual_labels = set(unique_labels.astype(int))\n",
        "\n",
        "if actual_labels == expected_labels:\n",
        "    print(f\"\\n✅ Label values match expected: {expected_labels}\")\n",
        "else:\n",
        "    unexpected = actual_labels - expected_labels\n",
        "    missing = expected_labels - actual_labels\n",
        "    print(f\"\\n⚠️ Label mismatch:\")\n",
        "    if unexpected:\n",
        "        print(f\"    Unexpected labels: {unexpected}\")\n",
        "    if missing:\n",
        "        print(f\"    Missing labels: {missing}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Categorical Feature Validation\n",
        "\n",
        "Verify that categorical features (book_valid, time_regime, mbo_ready, schema_version) have expected values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Categorical feature indices and expected values\n",
        "categorical_features = {\n",
        "    'book_valid': {\n",
        "        'index': FeatureIndex.BOOK_VALID,\n",
        "        'expected_values': {0.0, 1.0},\n",
        "        'description': 'Safety gate: 1=valid book, 0=crossed/empty'\n",
        "    },\n",
        "    'time_regime': {\n",
        "        'index': FeatureIndex.TIME_REGIME,\n",
        "        'expected_values': {0.0, 1.0, 2.0, 3.0, 4.0},\n",
        "        'description': '0=Open, 1=Early, 2=Midday, 3=Close, 4=Closed'\n",
        "    },\n",
        "    'mbo_ready': {\n",
        "        'index': FeatureIndex.MBO_READY,\n",
        "        'expected_values': {0.0, 1.0},\n",
        "        'description': 'Warmup gate: 1=ready, 0=warming up'\n",
        "    },\n",
        "    'invalidity_delta': {\n",
        "        'index': FeatureIndex.INVALIDITY_DELTA,\n",
        "        'expected_values': None,  # Count, any non-negative\n",
        "        'description': 'Count of feed problems since last sample'\n",
        "    },\n",
        "    'schema_version': {\n",
        "        'index': FeatureIndex.SCHEMA_VERSION_FEATURE,\n",
        "        'expected_values': {float(SCHEMA_VERSION)},\n",
        "        'description': f'Schema version constant: {SCHEMA_VERSION}'\n",
        "    },\n",
        "}\n",
        "\n",
        "print(\"Categorical Feature Validation:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for name, info in categorical_features.items():\n",
        "    col = train_features[:, info['index']]\n",
        "    unique_vals = set(np.unique(col))\n",
        "    \n",
        "    print(f\"\\n{name} (index {info['index']}):\")\n",
        "    print(f\"  Description: {info['description']}\")\n",
        "    print(f\"  Unique values: {sorted(unique_vals)}\")\n",
        "    \n",
        "    if info['expected_values'] is not None:\n",
        "        if unique_vals == info['expected_values']:\n",
        "            print(f\"  ✅ Matches expected values\")\n",
        "        elif unique_vals.issubset(info['expected_values']):\n",
        "            missing = info['expected_values'] - unique_vals\n",
        "            print(f\"  ⚠️ Subset of expected (missing: {missing})\")\n",
        "        else:\n",
        "            unexpected = unique_vals - info['expected_values']\n",
        "            print(f\"  ❌ Unexpected values: {unexpected}\")\n",
        "    \n",
        "    # Value distribution\n",
        "    for val in sorted(unique_vals):\n",
        "        count = (col == val).sum()\n",
        "        pct = 100 * count / len(col)\n",
        "        print(f\"    {val}: {count:,} ({pct:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Signal Feature Analysis\n",
        "\n",
        "Detailed analysis of the 14 trading signals (indices 84-97).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Signal feature names and indices\n",
        "signal_info = {\n",
        "    84: ('true_ofi', 'Cont et al. OFI', 'continuous'),\n",
        "    85: ('depth_norm_ofi', 'OFI / avg_depth', 'continuous'),\n",
        "    86: ('executed_pressure', 'trades_ask - trades_bid', 'continuous'),\n",
        "    87: ('signed_mp_delta_bps', 'Microprice deviation', 'continuous'),\n",
        "    88: ('trade_asymmetry', '(ask-bid)/total trades', 'continuous'),\n",
        "    89: ('cancel_asymmetry', '(ask-bid)/total cancels', 'continuous'),\n",
        "    90: ('fragility_score', 'concentration/ln(depth)', 'continuous'),\n",
        "    91: ('depth_asymmetry', '(bid-ask)/total depth', 'continuous'),\n",
        "    92: ('book_valid', 'Safety gate', 'binary'),\n",
        "    93: ('time_regime', 'Market session', 'categorical'),\n",
        "    94: ('mbo_ready', 'Warmup flag', 'binary'),\n",
        "    95: ('dt_seconds', 'Sample duration', 'continuous'),\n",
        "    96: ('invalidity_delta', 'Feed problems', 'count'),\n",
        "    97: ('schema_version', 'Version constant', 'constant'),\n",
        "}\n",
        "\n",
        "# Compute detailed statistics for each signal\n",
        "signal_stats = []\n",
        "\n",
        "for idx, (name, description, dtype) in signal_info.items():\n",
        "    col = train_features[:, idx]\n",
        "    \n",
        "    stats = {\n",
        "        'index': idx,\n",
        "        'name': name,\n",
        "        'type': dtype,\n",
        "        'mean': col.mean(),\n",
        "        'std': col.std(),\n",
        "        'min': col.min(),\n",
        "        'max': col.max(),\n",
        "        'median': np.median(col),\n",
        "        'q25': np.percentile(col, 25),\n",
        "        'q75': np.percentile(col, 75),\n",
        "        'n_unique': len(np.unique(col)),\n",
        "    }\n",
        "    \n",
        "    # For continuous features, check for outliers (|z| > 4)\n",
        "    if dtype == 'continuous' and stats['std'] > 0:\n",
        "        z_scores = np.abs((col - stats['mean']) / stats['std'])\n",
        "        stats['pct_outliers'] = 100 * (z_scores > 4).mean()\n",
        "    else:\n",
        "        stats['pct_outliers'] = 0\n",
        "    \n",
        "    signal_stats.append(stats)\n",
        "\n",
        "df_signals = pd.DataFrame(signal_stats)\n",
        "print(\"Signal Feature Statistics:\")\n",
        "print(df_signals[['index', 'name', 'type', 'mean', 'std', 'min', 'max', 'pct_outliers']].to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Sanity Checks: Known Relationships\n",
        "\n",
        "Verify known relationships still hold after normalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# After Z-score normalization, we check correlation patterns and sign conventions\n",
        "print(\"Sanity Check: Signal Relationships\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# true_ofi and depth_norm_ofi should be highly correlated (same base signal)\n",
        "true_ofi = train_features[:, 84]\n",
        "depth_norm_ofi = train_features[:, 85]\n",
        "corr_ofi = np.corrcoef(true_ofi, depth_norm_ofi)[0, 1]\n",
        "print(f\"\\n1. true_ofi ↔ depth_norm_ofi correlation: {corr_ofi:.4f}\")\n",
        "if corr_ofi > 0.7:\n",
        "    print(\"   ✅ High correlation (expected: same base signal)\")\n",
        "else:\n",
        "    print(\"   ⚠️ Lower than expected correlation\")\n",
        "\n",
        "# trade_asymmetry and executed_pressure should be positively correlated\n",
        "trade_asym = train_features[:, 88]\n",
        "exec_pressure = train_features[:, 86]\n",
        "corr_trade = np.corrcoef(trade_asym, exec_pressure)[0, 1]\n",
        "print(f\"\\n2. trade_asymmetry ↔ executed_pressure correlation: {corr_trade:.4f}\")\n",
        "if corr_trade > 0.3:\n",
        "    print(\"   ✅ Positive correlation (expected: both measure trade imbalance)\")\n",
        "else:\n",
        "    print(\"   ⚠️ Lower than expected correlation\")\n",
        "\n",
        "# book_valid should be 1 for most samples (data quality)\n",
        "book_valid = train_features[:, 92]\n",
        "valid_pct = (book_valid == 1.0).mean() * 100\n",
        "print(f\"\\n3. book_valid = 1: {valid_pct:.2f}%\")\n",
        "if valid_pct > 95:\n",
        "    print(\"   ✅ Most samples have valid book (expected for quality data)\")\n",
        "elif valid_pct > 80:\n",
        "    print(\"   ⚠️ Some invalid book samples\")\n",
        "else:\n",
        "    print(\"   ❌ Many invalid samples - data quality issue\")\n",
        "\n",
        "# mbo_ready should be 1 for most samples (after warmup)\n",
        "mbo_ready = train_features[:, 94]\n",
        "ready_pct = (mbo_ready == 1.0).mean() * 100\n",
        "print(f\"\\n4. mbo_ready = 1: {ready_pct:.2f}%\")\n",
        "if ready_pct > 95:\n",
        "    print(\"   ✅ Most samples are warmed up (expected)\")\n",
        "else:\n",
        "    print(\"   ⚠️ Significant warmup period\")\n",
        "\n",
        "# schema_version should be constant at 2\n",
        "schema = train_features[:, 97]\n",
        "unique_schema = np.unique(schema)\n",
        "print(f\"\\n5. schema_version values: {unique_schema}\")\n",
        "if len(unique_schema) == 1 and unique_schema[0] == SCHEMA_VERSION:\n",
        "    print(f\"   ✅ Constant at {SCHEMA_VERSION} (expected)\")\n",
        "else:\n",
        "    print(\"   ❌ Schema version inconsistency!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary\n",
        "\n",
        "Complete data profile for downstream analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate statistics across all splits\n",
        "total_samples = sum([d['features'].shape[0] for split in SPLITS for d in all_data[split]])\n",
        "total_labels = sum([d['labels'].shape[0] for split in SPLITS for d in all_data[split]])\n",
        "total_days = sum([len(all_data[split]) for split in SPLITS])\n",
        "\n",
        "train_days = len(all_data['train'])\n",
        "val_days = len(all_data['val'])\n",
        "test_days = len(all_data['test'])\n",
        "\n",
        "val_labels = np.concatenate([d['labels'] for d in all_data['val']])\n",
        "test_labels = np.concatenate([d['labels'] for d in all_data['test']])\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"DATASET PROFILE SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nSymbol: NVDA\")\n",
        "print(f\"Date range: {all_data['train'][0]['date']} to {all_data['test'][-1]['date']}\")\n",
        "print(f\"Total trading days: {total_days}\")\n",
        "\n",
        "print(f\"\\n--- Sample Counts ---\")\n",
        "print(f\"Total feature samples: {total_samples:,}\")\n",
        "print(f\"Total labels: {total_labels:,}\")\n",
        "print(f\"Feature/Label ratio: {total_samples/total_labels:.2f}\")\n",
        "\n",
        "print(f\"\\n--- Split Distribution ---\")\n",
        "print(f\"Train: {train_days} days, {len(train_labels):,} labels ({100*len(train_labels)/total_labels:.1f}%)\")\n",
        "print(f\"Val:   {val_days} days, {len(val_labels):,} labels ({100*len(val_labels)/total_labels:.1f}%)\")\n",
        "print(f\"Test:  {test_days} days, {len(test_labels):,} labels ({100*len(test_labels)/total_labels:.1f}%)\")\n",
        "\n",
        "print(f\"\\n--- Label Distribution (Train) ---\")\n",
        "for lbl in [LABEL_DOWN, LABEL_STABLE, LABEL_UP]:\n",
        "    count = (train_labels == lbl).sum()\n",
        "    print(f\"{LABEL_NAMES[lbl]:8s}: {count:6,} ({100*count/len(train_labels):5.2f}%)\")\n",
        "\n",
        "print(f\"\\n--- Data Quality ---\")\n",
        "print(f\"NaN values: {nan_count} ({100*nan_count/train_features.size:.6f}%)\")\n",
        "print(f\"Inf values: {inf_count} ({100*inf_count/train_features.size:.6f}%)\")\n",
        "print(f\"book_valid=1: {valid_pct:.2f}%\")\n",
        "print(f\"mbo_ready=1: {ready_pct:.2f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✅ DATA OVERVIEW COMPLETE - Ready for analysis\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save summary for other notebooks\n",
        "import os\n",
        "os.makedirs('../docs/figures', exist_ok=True)\n",
        "\n",
        "summary = {\n",
        "    'total_samples': int(total_samples),\n",
        "    'total_labels': int(total_labels),\n",
        "    'total_days': total_days,\n",
        "    'train_days': train_days,\n",
        "    'val_days': val_days,\n",
        "    'test_days': test_days,\n",
        "    'feature_count': FEATURE_COUNT,\n",
        "    'label_distribution': {\n",
        "        'down': int((train_labels == LABEL_DOWN).sum()),\n",
        "        'stable': int((train_labels == LABEL_STABLE).sum()),\n",
        "        'up': int((train_labels == LABEL_UP).sum()),\n",
        "    },\n",
        "    'data_quality': {\n",
        "        'nan_count': int(nan_count),\n",
        "        'inf_count': int(inf_count),\n",
        "        'book_valid_pct': float(valid_pct),\n",
        "        'mbo_ready_pct': float(ready_pct),\n",
        "    },\n",
        "}\n",
        "\n",
        "with open('../docs/data_overview_summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(\"Summary saved to docs/data_overview_summary.json\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
