# =============================================================================
# EXPERIMENT: NVDA DeepLOB h=10 with Class Weights
# =============================================================================
#
# Experiment ID: nvda_h10_weighted_v1
# Date Created: 2025-12-28
#
# PURPOSE:
#   Baseline experiment for NVDA directional prediction using DeepLOB model
#   with class-weighted loss to handle natural class imbalance.
#
# HYPOTHESIS:
#   Class weights will improve directional accuracy by forcing the model to
#   pay more attention to minority classes, without artificially balancing
#   the labels (which would lose economic meaning).
#
# KEY METRICS TO TRACK:
#   - val_directional_accuracy: Primary metric (Up/Down accuracy only)
#   - val_up_precision: When predicting Up, how often correct?
#   - val_down_precision: When predicting Down, how often correct?
#   - val_signal_rate: How often we predict Up/Down vs Stable
#
# COMPARISON BASELINE:
#   Previous h=10 run without class weights achieved:
#   - test_accuracy: 59.87%
#   - test_directional_accuracy: ~71% (estimated from per-class metrics)
#   - Stable recall: 17% (model avoided predicting Stable)
#
# =============================================================================
# HORIZON INDEX MAPPING (nvda_balanced dataset)
# =============================================================================
# The dataset contains labels for 5 prediction horizons:
#
#   horizon_idx: 0  →  h=10   (~1 second ahead)    - THIS EXPERIMENT
#   horizon_idx: 1  →  h=20   (~2 seconds ahead)
#   horizon_idx: 2  →  h=50   (~5 seconds ahead)
#   horizon_idx: 3  →  h=100  (~10 seconds ahead)  - Paper benchmark
#   horizon_idx: 4  →  h=200  (~20 seconds ahead)
#
# Note: With event_count=1000 sampling, each event ≈ 0.1-0.5 seconds
# =============================================================================

name: nvda_h10_weighted_v1
description: |
  NVDA DeepLOB experiment with h=10 horizon and class-weighted loss.
  
  Goal: Establish baseline directional accuracy with proper handling of
  natural class imbalance via loss weighting rather than label manipulation.
  
  This uses the ORIGINAL export (quantile threshold) for comparison with
  previous runs. Future experiments may use fixed-threshold exports.

tags:
  - deeplob
  - nvda
  - h10
  - class-weights
  - directional-focus
  - experiment-v1

# =============================================================================
# Data Configuration
# =============================================================================
data:
  # Path to exported dataset (relative to lob-model-trainer directory)
  data_dir: "../data/exports/nvda_balanced"
  
  # Feature specification
  feature_count: 98  # Full feature set: LOB(40) + Derived(8) + MBO(36) + Signals(14)
  
  # Horizon selection (0-based index into horizons array [10, 20, 50, 100, 200])
  horizon_idx: 0  # h=10 events (~1 second ahead)
  
  # Sequence configuration (must match Rust export)
  sequence:
    window_size: 100  # 100 timesteps per sequence
    stride: 10        # Step between sequences
  
  # Normalization (data is pre-normalized in Rust, this is for reference)
  normalization:
    strategy: zscore_per_day
    eps: 1.0e-8
    clip_value: 10.0
    exclude_features: []  # Categorical features already handled in Rust
  
  # Label configuration
  label_encoding: categorical  # {0, 1, 2} = {Down, Stable, Up}
  num_classes: 3
  cache_in_memory: true

# =============================================================================
# Model Configuration - DeepLOB (Zhang et al. 2019)
# =============================================================================
model:
  model_type: deeplob
  
  # DeepLOB architecture (exact paper specification)
  deeplob_mode: benchmark      # Use 40 LOB features only
  deeplob_conv_filters: 32     # 32 filters per conv block
  deeplob_inception_filters: 64  # 64 filters per inception branch
  deeplob_lstm_hidden: 64      # 64 hidden units
  deeplob_num_levels: 10       # 10 LOB levels
  
  # Generic model params (required by schema)
  input_size: 40       # LOB features only in benchmark mode
  hidden_size: 64      # Matches LSTM hidden
  num_layers: 1
  dropout: 0.0         # Paper uses no dropout
  num_classes: 3       # Down / Stable / Up

# =============================================================================
# Training Configuration
# =============================================================================
train:
  # Batch and epochs
  batch_size: 64
  epochs: 10           # Quick iteration (extend if promising)
  
  # Optimizer
  learning_rate: 1.0e-4
  weight_decay: 0.0    # Paper doesn't use weight decay
  
  # KEY CHANGE: Enable class weights to handle imbalance
  # This computes inverse frequency weights: weight[c] = total / (3 * count[c])
  # Rare classes (like Stable at h=10) get higher weight
  use_class_weights: true
  
  # Regularization
  gradient_clip_norm: 1.0
  
  # Learning rate schedule
  scheduler: cosine
  
  # Early stopping
  early_stopping_patience: 5  # Stop if val_loss doesn't improve for 5 epochs
  
  # Data loading
  num_workers: 0       # Avoid multiprocessing issues on macOS
  pin_memory: true
  
  # Reproducibility
  seed: 42
  mixed_precision: false

# =============================================================================
# Output Configuration
# =============================================================================
output_dir: outputs/experiments/nvda_h10_weighted_v1
log_level: INFO

# =============================================================================
# Experiment Notes
# =============================================================================
# 
# CHANGES FROM BASELINE:
#   1. use_class_weights: true (was false)
#   2. epochs: 10 (was 50, reduced for quick iteration)
#   3. early_stopping_patience: 5 (was 10, faster feedback)
#
# EXPECTED OUTCOMES:
#   - Higher directional accuracy (more attention to Up/Down)
#   - Better Stable recall (won't ignore minority class)
#   - Potentially lower overall accuracy (tradeoff)
#
# NEXT EXPERIMENTS:
#   - nvda_h20_weighted_v1: Same setup with h=20
#   - nvda_h10_focal_v1: Focal loss instead of class weights
#   - nvda_h10_fixed_threshold_v1: Re-export with fixed threshold labels
#
# =============================================================================

