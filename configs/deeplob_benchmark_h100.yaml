# DeepLOB Benchmark - Horizon 100 (Paper Default)
#
# This configuration matches the DeepLOB paper's k=4 (horizon=100) setting.
#
# Reference:
#   Zhang, Zohren & Roberts (2019). "DeepLOB: Deep Convolutional Neural 
#   Networks for Limit Order Books." IEEE Trans. Signal Processing.
#
# Usage:
#   python scripts/train.py --config configs/deeplob_benchmark_h100.yaml
#
# =============================================================================
# HORIZON INDEX MAPPING (nvda_balanced dataset)
# =============================================================================
# The dataset contains labels for 5 prediction horizons:
#
#   horizon_idx: 0  →  h=10   (~1 second ahead)    - easiest, best class balance
#   horizon_idx: 1  →  h=20   (~2 seconds ahead)
#   horizon_idx: 2  →  h=50   (~5 seconds ahead)
#   horizon_idx: 3  →  h=100  (~10 seconds ahead)  - PAPER BENCHMARK (this config)
#   horizon_idx: 4  →  h=200  (~20 seconds ahead)  - hardest, worst class balance
#
# =============================================================================

name: deeplob_benchmark_nvda_h100
description: |
  DeepLOB benchmark model for NVDA price direction prediction.
  Horizon=100 events (~10 seconds ahead) - matches original paper's k=4 setting.
  
  Architecture (exact paper spec):
  - 3 Conv blocks (32 filters each)
  - Inception module (64 filters per branch, 192 total)
  - LSTM (64 hidden units)
  - Linear classifier (3 classes)

tags:
  - deeplob
  - benchmark
  - nvda
  - h100
  - paper-default

# =============================================================================
# Data Configuration
# =============================================================================
data:
  data_dir: "../data/exports/nvda_balanced"
  feature_count: 98  # Full dataset has 98, benchmark uses first 40
  
  # CRITICAL: horizon_idx=3 corresponds to h=100 (see mapping above)
  horizon_idx: 3
  
  sequence:
    window_size: 100  # Must match Rust export config
    stride: 10
  
  normalization:
    strategy: zscore_per_day
    eps: 1.0e-8
    clip_value: 10.0
    exclude_features: []
  
  label_encoding: categorical
  num_classes: 3
  cache_in_memory: true

# =============================================================================
# Model Configuration - DeepLOB (Zhang et al. 2019)
# =============================================================================
model:
  model_type: deeplob
  
  # DeepLOB-specific parameters (paper defaults)
  deeplob_mode: benchmark  # Use exact paper architecture
  deeplob_conv_filters: 32  # Paper: 32 filters per conv block
  deeplob_inception_filters: 64  # Paper: 64 filters per inception branch
  deeplob_lstm_hidden: 64  # Paper: 64 hidden units
  deeplob_num_levels: 10  # 10 LOB levels (standard)
  
  # Schema-required fields (not used for DeepLOB)
  input_size: 40
  hidden_size: 64
  num_layers: 1
  dropout: 0.0
  num_classes: 3

# =============================================================================
# Training Configuration
# =============================================================================
train:
  batch_size: 64
  learning_rate: 1.0e-4  # Conservative for NVDA data
  weight_decay: 0.0
  
  # Quick iteration on M1 Pro (10 epochs ≈ 2.5 hours)
  epochs: 10
  early_stopping_patience: 5
  
  gradient_clip_norm: 1.0
  scheduler: cosine
  num_workers: 0  # Required for macOS
  pin_memory: true
  seed: 42
  mixed_precision: false
  use_class_weights: false

# =============================================================================
# Output
# =============================================================================
output_dir: outputs/deeplob_h100
log_level: INFO

