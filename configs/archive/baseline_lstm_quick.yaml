# Quick LSTM Configuration for Fast Iteration
#
# Shorter training run for quick experimentation.
# Uses nvda_balanced dataset with quantile-based thresholds for balanced classes.
#
# Usage:
#   python scripts/train.py --config configs/baseline_lstm_quick.yaml

name: baseline_lstm_nvda_quick
description: |
  Quick baseline LSTM on balanced dataset.
  For fast iteration - runs 20 epochs only.
  
tags:
  - baseline
  - lstm
  - nvda
  - quick
  - balanced-classes

# Data Configuration
data:
  data_dir: "../data/exports/nvda_balanced"
  feature_count: 98
  horizon_idx: 0  # First horizon (10 steps)
  
  sequence:
    window_size: 100
    stride: 10
  
  normalization:
    strategy: zscore_per_day
    eps: 1.0e-8
    clip_value: 10.0
    exclude_features:
      - 93  # TIME_REGIME
  
  label_encoding: categorical
  num_classes: 3
  cache_in_memory: true

# Model Configuration
model:
  model_type: lstm
  input_size: 98
  hidden_size: 64
  num_layers: 2
  dropout: 0.2
  num_classes: 3
  lstm_bidirectional: false

# Training Configuration - QUICK
train:
  batch_size: 128          # Larger batch for faster training
  learning_rate: 1.0e-3    # Higher LR for faster convergence
  weight_decay: 1.0e-5
  epochs: 20               # Quick run
  early_stopping_patience: 5
  gradient_clip_norm: 1.0
  scheduler: cosine
  num_workers: 0           # Single process
  pin_memory: false
  seed: 42
  mixed_precision: false
  use_class_weights: false  # Balanced dataset - no need for class weights

# Output
output_dir: outputs/baseline_lstm_quick
log_level: INFO

