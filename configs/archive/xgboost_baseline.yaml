# XGBoost Baseline Configuration
#
# This configuration trains an XGBoost classifier on the 98-feature set.
# Uses flat features (no sequence windowing) for faster iteration.
#
# Usage:
#   python -m lobtrainer.train --config configs/xgboost_baseline.yaml

name: xgboost_baseline_nvda
description: |
  XGBoost baseline for NVDA price direction prediction.
  Uses flat features (last sample per sequence) rather than full sequences.
  Good for feature importance analysis and establishing baselines.
  
tags:
  - baseline
  - xgboost
  - nvda
  - flat-features

# Data Configuration
data:
  data_dir: "../data/exports/nvda_98feat"
  feature_count: 98
  
  sequence:
    window_size: 1  # Single sample (no windowing)
    stride: 1
  
  normalization:
    strategy: zscore_per_day
    eps: 1.0e-8
    clip_value: 10.0
    exclude_features:
      - 93  # TIME_REGIME
  
  label_encoding: categorical
  num_classes: 3
  cache_in_memory: true

# Model Configuration
model:
  model_type: xgboost
  input_size: 98
  hidden_size: 100      # n_estimators
  num_layers: 6         # max_depth
  dropout: 0.0          # Not used
  num_classes: 3

# Training Configuration
train:
  batch_size: 1024      # Larger batches for XGBoost
  learning_rate: 0.1    # XGBoost learning rate (eta)
  weight_decay: 0.0     # Not used
  epochs: 1             # XGBoost trains in single pass
  early_stopping_patience: 10
  gradient_clip_norm: null
  scheduler: none
  num_workers: 4
  pin_memory: false
  seed: 42
  mixed_precision: false

# Output
output_dir: outputs/xgboost_baseline
log_level: INFO

